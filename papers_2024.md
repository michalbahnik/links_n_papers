# Papers in 2024

These papers cought my attention in year 2024.

* [TinyLlama: An Open-Source Small Language Model](https://paperswithcode.com/paper/tinyllama-an-open-source-small-language-model)
  * 1.1B LLM based on LLama 2 with great performance/size ratio.
* [Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch](https://paperswithcode.com/paper/language-models-are-super-mario-absorbing)
  * Merging different homologous models without training by delta parameter manipulationby DARE method.
* [OpenVoice: Versatile Instant Voice Cloning](https://paperswithcode.com/paper/openvoice-versatile-instant-voice-cloning)
  * Open-source voice cloning. Zero-shot ross-language.
* [Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications]()
  * Improving Deformable Convolution v4 (DCNv4) operator, addressing limitations of DCNv3 with improved dynamic property, expressive power, and memory optimization.
* [The boundary of neural network trainability is fractal](https://arxiv.org/abs/2402.06184v1)
  * Funny paper discovering fractals in neural network training.
* [Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting](https://paperswithcode.com/paper/lag-llama-towards-foundation-models-for-time)
  * Foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture that uses lags as covariates.
* [Learning to Fly in Seconds](https://paperswithcode.com/paper/learning-to-fly-in-seconds)
  * Asymmetric actor-critic-based architecture coupled with a highly reliable RL-based training paradigm for efficient end-to-end quadrotor control learning.
* [Scalable Diffusion Models with Transformers](https://paperswithcode.com/paper/scalable-diffusion-models-with-transformers)
  * Introducing Diffusion Transformers (DiT) by replacing U-Net backbone in Diffusion models with Visual Transformers.
* [World Model on Million-Length Video And Language With RingAttention](https://paperswithcode.com/paper/world-model-on-million-length-video-and)
  * Training LLMs on long videos and texts with high retrieval rate and 1M context length, introducing RingAttention.
* [Revisiting Feature Prediction for Learning Visual Representations from Video](https://paperswithcode.com/paper/revisiting-feature-prediction-for-learning)
  * Self-supervised visual feature prediction on videos aims to improve real world universal understanding.
* [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/pdf/2403.03507v1.pdf)
  * Innovative full-parameter but memory efficient training strategy for LLMs.
* [TripoSR: Fast 3D Object Reconstruction from a Single Image](https://arxiv.org/abs/2403.02151)
  * Open-source (MIT) object reconstruction from a single (!) image.
* [Evolutionary Optimization of Model Merging Recipes](https://paperswithcode.com/paper/evolutionary-optimization-of-model-merging)
  * Effective cross-architecture LLM merging using evolutionary algorithms.
* [FeatUp: A Model-Agnostic Framework for Features at Any Resolution](https://paperswithcode.com/paper/featup-a-model-agnostic-framework-for)
  * Fast model-agnostic drop-in visual features upsampler.
* [LLM4Decompile: Decompiling Binary Code with Large Language Models](https://paperswithcode.com/paper/llm4decompile-decompiling-binary-code-with)
  * First open-source C code decompilation LLM and re-compilability and re-executability focused benchmark.
* [EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models](https://paperswithcode.com/paper/easyjailbreak-a-unified-framework-for)
  * Framework for automatic jailbraking of LLMs with nice overview of jailbraking methods.
* [UniDepth: Universal Monocular Metric Depth Estimation](https://paperswithcode.com/paper/unidepth-universal-monocular-metric-depth)
  * Universal monocular depth estimation with strong zero-shot capability.  
* [LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis](https://paperswithcode.com/paper/lidar4d-dynamic-neural-fields-for-novel-space)
  * Novel method to reconstruct scenes from lidar data using Neural Radience Fields.
* [From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples](https://paperswithcode.com/paper/from-words-to-numbers-your-large-language)
  * Paper analysis LLms in terms of regression capabilities and finds out, many models can beat algorithms like Random Forest.
* [KAN: Kolmogorov-Arnold Networks](https://arxiv.org/abs/2404.19756)
  * Interesting alternative to MLP: Move activation functions to edges and make them learnable, while dropping all linear weights.
* [Depth Anything V2](https://paperswithcode.com/paper/depth-anything-v2)
  * Efficient foundation monocular depth estimation discriminative model learnt with teacher-student paradigm and synthetic data. Also introduces DA-2K dataset.
* [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://paperswithcode.com/paper/deepseek-coder-v2-breaking-the-barrier-of)
  * Open-source model to surpass closed-source models in coding and math.
* [MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens](https://paperswithcode.com/paper/mint-1t-scaling-open-source-multimodal-data)
  * Large, multimodal open-source dataset with diverse data sources.
* [Neural General Circulation Models for Weather and Climate]([https://github.com/google-research/neuralgcm](https://paperswithcode.com/paper/neural-general-circulation-models)
  * Physics-based weather circulation simulators combining GCMs with ML components for efficient weather forcasting.
* [BitNet: Scaling 1-bit Transformers for Large Language Models](https://www.microsoft.com/en-us/research/publication/bitnet-scaling-1-bit-transformers-for-large-language-models/)
  * Scaling transformers to 1.58 bit (-1, 0, 1) to improve efficiency and power consumption with minimal accuracy loss.
